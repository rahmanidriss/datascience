{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AQE\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/basics-of-apache-spark-configuration-settings-ca4faff40d45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YARN Dependent Parameters\n",
    "yarn.nodemanager.resource.memory-mb\n",
    "yarn.scheduler.maximum-allocation-mb\n",
    "yarn.scheduler.minimum-allocation-mb\n",
    "yarn.nodemanager.resource.cpu-vcores\n",
    "yarn.scheduler.maximum-allocation-vcores\n",
    "yarn.scheduler.minimum-allocation-vcores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Management in Spark\n",
    "\n",
    "User Memory = (Heap Size-300MB)*(1-spark.memory.fraction)\n",
    "# where 300MB stands for reserved memory and spark.memory.fraction propery is 0.6 by default.\n",
    "Execution memory = Usable Memory * spark.memory.fraction*(1-spark.memory.storageFraction)\n",
    "Storage memory = Usable Memory * spark.memory.fraction*spark.memory.storageFraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"jvm.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Configuration Settings\n",
    "spark.executor.instances: Number of executors for the spark application.\n",
    "spark.executor.memory: Amount of memory to use for each executor that runs the task.\n",
    "spark.executor.cores: Number of concurrent tasks an executor can run.\n",
    "spark.driver.memory: Amount of memory to use for the driver.\n",
    "spark.driver.cores: Number of virtual cores to use for the driver process.\n",
    "spark.sql.shuffle.partitions: Number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.default.parallelism: Default number of partitions in resilient distributed datasets (RDDs) returned by transformations like join and aggregations.\n",
    "\n",
    "One vcore per node might be reserved for Hadoop and OS daemons\n",
    "1 GB per node might be reserved for Hadoop and OS daemons\n",
    "\n",
    "spark.executor.cores : 5\n",
    "executor_per_node = (vcore_per_node-1)/spark.executor.cores\n",
    "executor_per_node = (16–1)/5 = 3\n",
    "spark.executor.instances = (executor_per_node * number_of_nodes)-1 \n",
    "spark.executor.instances = (3*3)-1 = 8\n",
    "\n",
    "total_executor_memory =  (total_ram_per_node -1) / executor_per_node\n",
    "total_executor_memory = (64–1)/3 = 21(rounded down)\n",
    "spark.executor.memory = total_executor_memory * 0.9\n",
    "spark.executor.memory = 21*0.9 = 18 (rounded down)\n",
    "memory_overhead = 21*0.1 = 3 (rounded up)\n",
    "\n",
    "spark.driver.memory can be set as the same as spark.executor.memory,\n",
    "just like spark.driver.cores is set as the same as spark.executors.cores.\n",
    "\n",
    "spark.default.parallelism = spark.executor.instances * spark.executor.cores * 2\n",
    "spark.default.parallelism = 8 * 5 * 2 = 80\n",
    "\n",
    "In the case of data frames, spark.sql.shuffle.partitions\n",
    "can be set along with spark.default.parallelism property.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"node.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic allocation\n",
    "spark.dynamicAllocation.enabled  \n",
    "spark.dynamicAllocation.initialExecutors,\n",
    "spark.dynamicAllocation.minExecutors,\n",
    "spark.dynamicAllocation.maxExecutors\n",
    "spark.dynamicAllocation.shuffleTracking.enabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin/spark-submit --class org.apache.spark.examples.SparkPi\n",
    "--master spark://localhost:7077 \n",
    "--executor-memory 2G \n",
    "--num-executors 1 \n",
    "--conf spark.dynamicAllocation.maxExecutors=3\n",
    "--conf spark.executor.cores=1 \n",
    "--conf spark.dynamicAllocation.enabled=true\n",
    "--conf spark.dynamicAllocation.shuffleTracking.enabled=true examples/jars/spark-examples_2.12-3.0.0-preview2.jar 10000"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
