{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/rahmanidriss/bigdata/tools/spark\")\n",
    "#findspark.init()\n",
    "#import pyspark\n",
    "#import SparkSession\n",
    "from pyspark.sql import Row, SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/04 13:44:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/04 13:44:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/02/04 13:44:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/02/04 13:44:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-pro-de-rahmani-2.home:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>repartition_strategies</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9a750a9430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    #.master(\"spark://172.30.0.3:7077\")\n",
    "    #.config(\"spark.cores.max\", 12)\n",
    "    #.config(\"spark.executor.cores\", 2)\n",
    "    #.config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.sql.adaptive.enabled\",'false')\n",
    "    #.config('spark.sql.adaptive.coalescePartitions.initialPartitionNum', 24)\n",
    "    #.config('spark.sql.adaptive.coalescePartitions.parallelismFirst', 'false')\n",
    "    #.config('spark.sql.files.minPartitionNum', 1)\n",
    "    #.config('spark.sql.files.maxPartitionBytes', '500mb')\n",
    "    .appName('repartition_strategies')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vj/v7stj5l97kx94gggh88slgfh0000gn/T/ipykernel_20762/884838579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/bigdata/tools/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "#spark.conf.get(\"spark.executor.cores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import  StructType,StructField,StringType,DoubleType, IntegerType,TimestampType, _parse_datatype_string, _parse_datatype_json_string\n",
    "import pyspark.sql.functions as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_schema=StructType([StructField(c, IntegerType(), True) for c in df.columns])\n",
    "\n",
    "_schema=StructType([\n",
    "    StructField('MONTH', IntegerType(), True),\n",
    "    StructField('DAY', IntegerType(), True),\n",
    "    StructField('WEEKDAY', IntegerType(), True),\n",
    "    StructField('AIRLINE', StringType(), True),\n",
    "    StructField('ORG_AIR', StringType(), True),\n",
    "    StructField('DEST_AIR', StringType(), True),\n",
    "    StructField('SCHED_DEP', TimestampType(), True),\n",
    "    StructField('DEP_DELAY', DoubleType(), True),\n",
    "    StructField('AIR_TIME', DoubleType(), True),\n",
    "    StructField('DIST', IntegerType(), True), \n",
    "    StructField('SCHED_ARR', IntegerType(), True),\n",
    "    StructField('ARR_DELAY', DoubleType(), True),\n",
    "    StructField('DIVERTED', IntegerType(), True),\n",
    "    StructField('CANCELLED', IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=(\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .options(header=\"true\",inferSschema=\"false\",sep=\",\")\n",
    "    .schema(_schema)\n",
    "    .load(\"file:///Users/rahmanidriss/Desktop/dataScience/spark_hadoop_cluster/data/flights.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- WEEKDAY: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- ORG_AIR: string (nullable = true)\n",
      " |-- DEST_AIR: string (nullable = true)\n",
      " |-- SCHED_DEP: timestamp (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DIST: integer (nullable = true)\n",
      " |-- SCHED_ARR: integer (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+-------+--------+-------------------+---------+--------+----+---------+---------+--------+---------+\n",
      "|MONTH|DAY|WEEKDAY|AIRLINE|ORG_AIR|DEST_AIR|          SCHED_DEP|DEP_DELAY|AIR_TIME|DIST|SCHED_ARR|ARR_DELAY|DIVERTED|CANCELLED|\n",
      "+-----+---+-------+-------+-------+--------+-------------------+---------+--------+----+---------+---------+--------+---------+\n",
      "|    1|  1|      4|     WN|    LAX|     SLC|1625-01-01 00:00:00|     58.0|    94.0| 590|     1905|     65.0|       0|        0|\n",
      "|    1|  1|      4|     UA|    DEN|     IAD|               NULL|      7.0|   154.0|1452|     1333|    -13.0|       0|        0|\n",
      "|    1|  1|      4|     MQ|    DFW|     VPS|1305-01-01 00:00:00|     36.0|    85.0| 641|     1453|     35.0|       0|        0|\n",
      "+-----+---+-------+-------+-------+--------+-------------------+---------+--------+----+---------+---------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT DEST_AIR)|\n",
      "+------------------------+\n",
      "|                     271|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (df.agg(*(F.count_distinct(F.col(c)).alias(c) for c in df.columns))\n",
    "#  .show()\n",
    "# )\n",
    "\n",
    "(df.agg(F.count_distinct(\"DEST_AIR\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     data|\n",
      "+---------+\n",
      "|[1, 2, 3]|\n",
      "|      [1]|\n",
      "|       []|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "empty_df = spark.createDataFrame([(1,0)], [\"id\",\"value\"])\n",
    "df_new = empty_df.withColumn('value', F.explode(F.array_repeat(F.col(\"value\"), 1000000)))\n",
    "df_new=df_new.drop((\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add=spark.createDataFrame([(0,5000),(1,10000),(2,100000)],[\"id\",\"value\"])\n",
    "df_add=df_add.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_new.union(df_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Sort [partition_id#195 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(partition_id#195 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=326]\n",
      "   +- *(4) HashAggregate(keys=[partition_id#195], functions=[count(value#105L), min(value#105L), max(value#105L)])\n",
      "      +- Exchange hashpartitioning(partition_id#195, 200), ENSURE_REQUIREMENTS, [plan_id=322]\n",
      "         +- *(3) HashAggregate(keys=[partition_id#195], functions=[partial_count(value#105L), partial_min(value#105L), partial_max(value#105L)])\n",
      "            +- *(3) Project [value#105L, SPARK_PARTITION_ID() AS partition_id#195]\n",
      "               +- Exchange RoundRobinPartitioning(4), REPARTITION_BY_NUM, [plan_id=317]\n",
      "                  +- Union\n",
      "                     :- *(1) Generate explode(array_repeat(value#101L, 1000000)), false, [value#105L]\n",
      "                     :  +- *(1) Project [value#101L]\n",
      "                     :     +- *(1) Scan ExistingRDD[id#100L,value#101L]\n",
      "                     +- *(2) Project [value#110L]\n",
      "                        +- *(2) Scan ExistingRDD[id#109L,value#110L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_robin_partition=df_final.repartition(4)\n",
    "round_robin_partition = (round_robin_partition\n",
    "# here I will insert the partition logic\n",
    "    .withColumn(\"partition_id\", F.spark_partition_id()) # applying SQL built-in function to determine actual partition\n",
    "    .groupBy(F.col(\"partition_id\"))\n",
    "    .agg(\n",
    "      F.count(F.col(\"value\")).alias(\"count\"),\n",
    "      F.min(F.col(\"value\")).alias(\"min_value\"),\n",
    "      F.max(F.col(\"value\")).alias(\"max_value\")\n",
    "      )\n",
    "    .orderBy(F.asc(F.col(\"partition_id\")))\n",
    ")\n",
    "round_robin_partition.explain()\n",
    "round_robin_partition.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'(4) MapPartitionsRDD[193] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[192] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[191] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[190] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  ShuffledRowRDD[189] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n +-(200) MapPartitionsRDD[188] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n     |   MapPartitionsRDD[184] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n     |   ShuffledRowRDD[183] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n     +-(8) MapPartitionsRDD[182] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n        |  MapPartitionsRDD[181] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n        |  ShuffledRowRDD[180] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n        +-(24) MapPartitionsRDD[179] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n           |   UnionRDD[178] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n           |   MapPartitionsRDD[176] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n           |   MapPartitionsRDD[8] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\\n           |   MapPartitionsRDD[7] at map at SerDeUtil.scala:69 []\\n           |   MapPartitionsRDD[6] at mapPartitions at SerDeUtil.scala:117 []\\n           |   PythonRDD[5] at RDD at PythonRDD.scala:53 []\\n           |   ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD.scala:289 []\\n           |   MapPartitionsRDD[177] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n           |   MapPartitionsRDD[13] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\\n           |   MapPartitionsRDD[12] at map at SerDeUtil.scala:69 []\\n           |   MapPartitionsRDD[11] at mapPartitions at SerDeUtil.scala:117 []\\n           |   PythonRDD[10] at RDD at PythonRDD.scala:53 []\\n           |   ParallelCollectionRDD[9] at readRDDFromFile at PythonRDD.scala:289 []'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_partition=df_final.repartition(8, F.col(\"value\"))\n",
    "hash_partition = (hash_partition\n",
    "# here I will insert the partition logic\n",
    "    .withColumn(\"partition_id\", F.spark_partition_id()) # applying SQL built-in function to determine actual partition\n",
    "    .groupBy(F.col(\"partition_id\"))\n",
    "    .agg(\n",
    "      F.count(F.col(\"value\")).alias(\"count\"),\n",
    "      F.min(F.col(\"value\")).alias(\"min_value\"),\n",
    "      F.max(F.col(\"value\")).alias(\"max_value\")\n",
    "      )\n",
    "    .orderBy(F.asc(F.col(\"partition_id\")))\n",
    ")\n",
    "hash_partition.show()\n",
    "print(hash_partition.rdd.getNumPartitions())\n",
    "print(hash_partition.count())\n",
    "#hash_partition.rdd.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+---------+---------+\n",
      "|partition_id|  count|min_value|max_value|\n",
      "+------------+-------+---------+---------+\n",
      "|           0|1000000|        0|        0|\n",
      "|           1|      1|     5000|     5000|\n",
      "|           2|      1|    10000|    10000|\n",
      "|           3|      1|   100000|   100000|\n",
      "+------------+-------+---------+---------+\n",
      "\n",
      "4\n",
      "4\n",
      "+------+--------+\n",
      "| value|count(1)|\n",
      "+------+--------+\n",
      "|     0| 1000000|\n",
      "| 10000|       1|\n",
      "|100000|       1|\n",
      "|  5000|       1|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Due to performance reasons this method uses sampling to estimate \n",
    "the ranges. Hence, the output may not be consistent,\n",
    "since sampling can return different values. \n",
    "The sample size can be controlled by the \n",
    "config spark.sql.execution.rangeExchange.sampleSizePerPartition.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "range_partition=df_final.repartitionByRange(8, F.col(\"value\"))\n",
    "range_partition = (range_partition\n",
    "# here I will insert the partition logic\n",
    "    .withColumn(\"partition_id\", F.spark_partition_id()) # applying SQL built-in function to determine actual partition\n",
    "    .groupBy(F.col(\"partition_id\"))\n",
    "    .agg(\n",
    "      F.count(F.lit(\"value\")).alias(\"count\"),\n",
    "      #F.count(F.lit(1)).alias(\"count\"),\n",
    "      F.min(F.col(\"value\")).alias(\"min_value\"),\n",
    "      F.max(F.col(\"value\")).alias(\"max_value\")\n",
    "      )\n",
    "    .orderBy(F.asc(F.col(\"partition_id\")))\n",
    ")\n",
    "range_partition.show()\n",
    "print(range_partition.rdd.getNumPartitions())\n",
    "print(range_partition.count())\n",
    "\n",
    "#df_final.groupBy(\"value\").agg(F.count(F.lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'CAST((rand(7973342854009252421) * 200) AS INT)'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F.rand() * spark.conf.get(\"spark.sql.shuffle.partitions\")).cast(\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------+\n",
      "|value|salt|partition_id|\n",
      "+-----+----+------------+\n",
      "|    0|  23|          11|\n",
      "|    0| 130|          11|\n",
      "|    0|  14|          11|\n",
      "|    0| 132|          11|\n",
      "|    0|  91|          11|\n",
      "|    0|  35|          11|\n",
      "|    0| 188|          11|\n",
      "|    0| 129|          11|\n",
      "|    0| 183|          11|\n",
      "|    0| 138|          11|\n",
      "|    0|  25|          11|\n",
      "|    0| 196|          11|\n",
      "|    0|  68|          11|\n",
      "|    0|  16|          11|\n",
      "|    0|  77|          11|\n",
      "|    0| 145|          11|\n",
      "|    0|  47|          11|\n",
      "|    0|  90|          11|\n",
      "|    0|  38|          11|\n",
      "|    0| 157|          11|\n",
      "+-----+----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left = (df_final.withColumn(\"salt\", (F.rand() * spark.conf.get(\"spark.sql.shuffle.partitions\")).cast(\"int\"))\n",
    ".withColumn(\"partition_id\",F.spark_partition_id()).show()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
