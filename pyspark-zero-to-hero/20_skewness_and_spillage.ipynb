{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0622da-9229-4822-8482-b9b76cd3b107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/04 22:14:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/04 22:14:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/02/04 22:14:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/02/04 22:14:02 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-pro-de-rahmani.home:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimizing Skewness and Spillage</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe3c3295550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Skewness and Spillage\")\n",
    "    #.master(\"spark://197e20b418a6:7077\")\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bccfeee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import  StructType,StructField,StringType,DoubleType, IntegerType,TimestampType, _parse_datatype_string, _parse_datatype_json_string\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99724543-fea0-4b89-996d-cf8cea168bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9191ed11-3ca0-4c15-a3ba-a80bc6fbf9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Employee data\n",
    "#_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "_schema =\"employee_id int,first_name string ,last_name string,email string,phone string ,hire_date Timestamp,job_id int ,salary double, commition_cp string, manager_id int, departement_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", False).load(\"file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa1e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "emp=emp.filter(emp.departement_id.isNotNull())\n",
    "#emp.groupBy(\"departement_id\").agg(F.count(F.lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d8df89-6d6a-4e10-99d5-560f8ea0b3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "_dept_schema = \"departement_id int, departement_name string,manager_id int ,location_id int\"\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba90ff0-a0dc-4a20-bd84-c2389d4ca147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "\n",
    "df_joined = emp.join(dept, on=emp.departement_id==dept.departement_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b613ce-a32a-44d4-bba2-1fa88753df0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/04 22:14:29 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60d15de-b612-433b-8775-6f5f88b27da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [departement_id#10], [departement_id#23], LeftOuter\n",
      ":- *(2) Sort [departement_id#10 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(departement_id#10, 200), ENSURE_REQUIREMENTS, [plan_id=94]\n",
      ":     +- *(1) Filter isnotnull(departement_id#10)\n",
      ":        +- FileScan csv [employee_id#0,first_name#1,last_name#2,email#3,phone#4,hire_date#5,job_id#6,salary#7,commition_cp#8,manager_id#9,departement_id#10] Batched: false, DataFilters: [isnotnull(departement_id#10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/p..., PartitionFilters: [], PushedFilters: [IsNotNull(departement_id)], ReadSchema: struct<employee_id:int,first_name:string,last_name:string,email:string,phone:string,hire_date:tim...\n",
      "+- *(4) Sort [departement_id#23 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(departement_id#23, 200), ENSURE_REQUIREMENTS, [plan_id=102]\n",
      "      +- *(3) Filter isnotnull(departement_id#23)\n",
      "         +- FileScan csv [departement_id#23,departement_name#24,manager_id#25,location_id#26] Batched: false, DataFilters: [isnotnull(departement_id#23)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/p..., PartitionFilters: [], PushedFilters: [IsNotNull(departement_id)], ReadSchema: struct<departement_id:int,departement_name:string,manager_id:int,location_id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Explain Plan\n",
    "\n",
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50625a07-db99-4803-a39e-68707085bfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/04 22:18:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|partition_num|count|\n",
      "+-------------+-----+\n",
      "|           53|    3|\n",
      "|          103|    6|\n",
      "|          122|    1|\n",
      "|          164|    2|\n",
      "|          136|    1|\n",
      "|          181|    6|\n",
      "|          150|    8|\n",
      "|          124|   23|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit\n",
    "\n",
    "part_df = df_joined.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62d80db9-18c9-459e-a1a5-8b395184dde2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|departement_id|count(1)|\n",
      "+--------------+--------+\n",
      "|            20|       2|\n",
      "|            40|       1|\n",
      "|           100|       6|\n",
      "|            10|       1|\n",
      "|            50|      23|\n",
      "|            70|       1|\n",
      "|            90|       3|\n",
      "|            60|       5|\n",
      "|           110|       2|\n",
      "|            30|       6|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify Employee data based on department_id\n",
    "from pyspark.sql.functions import count, lit, desc, col\n",
    "\n",
    "emp.groupBy(\"departement_id\").agg(count(lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "409a7d44-5f4f-4301-9d4f-6e627334d529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set shuffle partitions to a lesser number - 16\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5d20825-28f4-4fef-99a8-9c65a64f73e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let prepare the salt\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to return a random number every time and add to Employee as salt\n",
    "@udf\n",
    "def salt_udf():\n",
    "    return random.randint(0, 32)\n",
    "\n",
    "# Salt Data Frame to add to department\n",
    "salt_df = spark.range(0, 32)\n",
    "\n",
    "#salt_df.withColumn(\"random\", F.round(F.rand()*(32),0).cast(IntegerType())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a51cc0a-43bd-4cba-8996-8eece7d97045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+------+-------+------------+----------+--------------+--------------+\n",
      "|employee_id|first_name|last_name|   email|       phone|hire_date|job_id| salary|commition_cp|manager_id|departement_id|salted_dept_id|\n",
      "+-----------+----------+---------+--------+------------+---------+------+-------+------------+----------+--------------+--------------+\n",
      "|        198|    Donald| OConnell|DOCONNEL|650.507.9833|     NULL|  NULL| 2600.0|          - |       124|            50|         50_19|\n",
      "|        199|   Douglas|    Grant|  DGRANT|650.507.9844|     NULL|  NULL| 2600.0|          - |       124|            50|         50_22|\n",
      "|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|     NULL|  NULL| 4400.0|          - |       101|            10|         10_23|\n",
      "|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|     NULL|  NULL|13000.0|          - |       100|            20|          20_5|\n",
      "|        202|       Pat|      Fay|    PFAY|603.123.6666|     NULL|  NULL| 6000.0|          - |       201|            20|          20_7|\n",
      "|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|     NULL|  NULL| 6500.0|          - |       101|            40|         40_27|\n",
      "|        204|   Hermann|     Baer|   HBAER|515.123.8888|     NULL|  NULL|10000.0|          - |       101|            70|         70_16|\n",
      "|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|     NULL|  NULL|12008.0|          - |       101|           110|         110_5|\n",
      "|        206|   William|    Gietz|  WGIETZ|515.123.8181|     NULL|  NULL| 8300.0|          - |       205|           110|        110_18|\n",
      "|        100|    Steven|     King|   SKING|515.123.4567|     NULL|  NULL|24000.0|          - |      NULL|            90|         90_26|\n",
      "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|     NULL|  NULL|17000.0|          - |       100|            90|         90_27|\n",
      "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|     NULL|  NULL|17000.0|          - |       100|            90|         90_13|\n",
      "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|     NULL|  NULL| 9000.0|          - |       102|            60|          60_3|\n",
      "|        104|     Bruce|    Ernst|  BERNST|590.423.4568|     NULL|  NULL| 6000.0|          - |       103|            60|          60_4|\n",
      "|        105|     David|   Austin| DAUSTIN|590.423.4569|     NULL|  NULL| 4800.0|          - |       103|            60|         60_24|\n",
      "|        106|     Valli|Pataballa|VPATABAL|590.423.4560|     NULL|  NULL| 4800.0|          - |       103|            60|         60_18|\n",
      "|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|     NULL|  NULL| 4200.0|          - |       103|            60|          60_5|\n",
      "|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|     NULL|  NULL|12008.0|          - |       101|           100|        100_24|\n",
      "|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|     NULL|  NULL| 9000.0|          - |       108|           100|        100_13|\n",
      "|        110|      John|     Chen|   JCHEN|515.124.4269|     NULL|  NULL| 8200.0|          - |       108|           100|        100_11|\n",
      "+-----------+----------+---------+--------+------------+---------+------+-------+------------+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted Employee\n",
    "from pyspark.sql.functions import lit, concat\n",
    "#F.round(F.rand()*(32),0).cast(IntegerType())\n",
    "salted_emp = emp.withColumn(\"salted_dept_id\", concat(\"departement_id\", lit(\"_\"), salt_udf()))\n",
    "\n",
    "salted_emp.show()                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3ba7f520-7b49-4473-9171-2b4281bbd3da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salted Department\n",
    "\n",
    "salted_dept = dept.join(salt_df, how=\"cross\").withColumn(\"salted_dept_id\", concat(\"departement_id\", lit(\"_\"), \"id\"))\n",
    "\n",
    "#salted_dept.where(\"departement_id = 50\").show()\n",
    "salted_dept.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94a09602-ccb0-44df-957e-a5ca7b193805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets make the salted join now\n",
    "salted_joined_df = salted_emp.join(salted_dept, on=salted_emp.salted_dept_id==salted_dept.salted_dept_id, how=\"left_outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edfbd1c8-5617-4a4b-9c88-97de98079271",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID, DEPARTMENT_NAME, MANAGER_ID, LOCATION_ID\n",
      " Schema: departement_id, departement_name, manager_id, location_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n"
     ]
    }
   ],
   "source": [
    "salted_joined_df.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ebe5989-fe90-4d6b-9645-6534ba497354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n",
      "24/02/05 10:09:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEPARTMENT_ID\n",
      " Schema: departement_id\n",
      "Expected: departement_id but found: DEPARTMENT_ID\n",
      "CSV file: file:///Users/rahmanidriss/Desktop/dataScience/spark_cluster/notebooks/pyspark-zero-to-hero/datasets/departments.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|partition_num|count|\n",
      "+-------------+-----+\n",
      "|           12|    4|\n",
      "|           18|    1|\n",
      "|           10|    6|\n",
      "|            1|    2|\n",
      "|           20|    3|\n",
      "|           29|    1|\n",
      "|           13|    1|\n",
      "|            6|    1|\n",
      "|            9|    1|\n",
      "|           23|    1|\n",
      "|            7|    2|\n",
      "|           11|    3|\n",
      "|           26|    1|\n",
      "|           30|    2|\n",
      "|           28|    1|\n",
      "|            0|    2|\n",
      "|           24|    2|\n",
      "|           16|    1|\n",
      "|            5|    1|\n",
      "|           22|    4|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "\n",
    "part_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a30325-65a3-4c65-a499-38147e42ce50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
